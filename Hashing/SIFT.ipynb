{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import shutil\n",
    "from itertools import combinations\n",
    "\n",
    "california_images = \"../California/photos\"\n",
    "custom_images = \"../My_Image\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SIFT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_duplicates_sift(folder_path, threshold, output_folder):\n",
    "    images_dict = {}  # Store image paths and their descriptors\n",
    "    duplicate_groups = {}\n",
    "    total_images = 0\n",
    "\n",
    "    for image_path in Path(folder_path).glob('*.*'):\n",
    "        if image_path.suffix.lower() in ['.jpg', '.jpeg', '.png']:\n",
    "            img = cv2.imread(str(image_path), cv2.IMREAD_GRAYSCALE)  # Load image as grayscale\n",
    "            if img is None:\n",
    "                print(f\"Unable to read image: {image_path}\")\n",
    "                continue\n",
    "            sift = cv2.SIFT_create()\n",
    "\n",
    "            # Find keypoints and calculate descriptors\n",
    "            keypoints, descriptors = sift.detectAndCompute(img, None)\n",
    "            if descriptors is None:\n",
    "                print(f\"No descriptors found for image: {image_path}\")\n",
    "                continue\n",
    "\n",
    "            images_dict[image_path] = descriptors\n",
    "            total_images += 1\n",
    "\n",
    "    # Compare descriptors using FLANN Matcher\n",
    "    flann_params = dict(algorithm=1, trees=5)\n",
    "    matcher = cv2.FlannBasedMatcher(flann_params, {})\n",
    "\n",
    "    duplicate_image_paths = set()\n",
    "    output_dir = os.path.join(output_folder)  # Create output folder\n",
    "    # Create the output folder if it doesn't exist\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "\n",
    "    for (path_a, desc_a), (path_b, desc_b) in combinations(images_dict.items(), 2):\n",
    "        try:\n",
    "            matches = matcher.knnMatch(desc_a, desc_b, k=2)\n",
    "        except cv2.error as e:\n",
    "            print(f\"Error matching descriptors for {path_a} and {path_b}: {e}\")\n",
    "            continue\n",
    "\n",
    "        # 'Lowe's ratio test' for robust matching\n",
    "        good_matches = [m for m, n in matches if m.distance < threshold * n.distance]\n",
    "\n",
    "        if len(good_matches) > 5:  # Reduced from 10 to 5\n",
    "            duplicate_groups.setdefault(path_a, []).append(path_b)\n",
    "            duplicate_image_paths.add(path_b)  # Add only duplicates, not the original\n",
    "            # Move duplicates to the output folder\n",
    "            shutil.copy(path_b, os.path.join(output_dir, os.path.basename(path_b)))\n",
    "\n",
    "    return duplicate_groups, duplicate_image_paths, total_images\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SURF / ORB (alternate of SURF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## currently not open source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_duplicates_surf(folder_path, threshold=0.7, output_folder=\"duplicates\"):\n",
    "    images_dict = {}  # Store image paths and their descriptors\n",
    "    duplicate_groups = {}\n",
    "\n",
    "    for image_path in Path(folder_path).glob('*.*'):\n",
    "        if image_path.suffix.lower() in ['.jpg', '.jpeg', '.png']:\n",
    "            img = cv2.imread(str(image_path), cv2.IMREAD_GRAYSCALE)  # Load image as grayscale\n",
    "            surf = cv2.xfeatures2d.SURF_create()\n",
    "\n",
    "            # Find keypoints and calculate descriptors\n",
    "            keypoints, descriptors = surf.detectAndCompute(img, None)\n",
    "\n",
    "            images_dict[image_path] = descriptors\n",
    "\n",
    "    # Compare descriptors using FLANN Matcher\n",
    "    flann_params = dict(algorithm=1, trees=5)\n",
    "    matcher = cv2.FlannBasedMatcher(flann_params, {})\n",
    "\n",
    "    duplicate_image_paths = set()\n",
    "\n",
    "    output_dir = os.path.join(folder_path, output_folder)\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "\n",
    "    for (path_a, desc_a), (path_b, desc_b) in combinations(images_dict.items(), 2):\n",
    "        matches = matcher.knnMatch(desc_a, desc_b, k=2)\n",
    "\n",
    "        # 'Lowe's ratio test' for robust matching\n",
    "        good_matches = [m for m, n in matches if m.distance < threshold * n.distance]\n",
    "\n",
    "        if len(good_matches) > 10:\n",
    "            duplicate_groups.setdefault(path_a, []).append(path_b)\n",
    "            duplicate_image_paths.add(path_b)  # Add only duplicates, not the original\n",
    "            # Move duplicates to the output folder\n",
    "            shutil.move(path_b, os.path.join(output_dir, os.path.basename(path_b)))\n",
    "\n",
    "    return duplicate_groups, duplicate_image_paths\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.image as mpimg\n",
    "\n",
    "def display_all_duplicate_images(folder_path):\n",
    "    image_files = [f for f in os.listdir(folder_path) if os.path.isfile(os.path.join(folder_path, f))]\n",
    "    for image_file in image_files:\n",
    "        image_path = os.path.join(folder_path, image_file)\n",
    "        img_data = mpimg.imread(image_path)\n",
    "        plt.figure()\n",
    "        plt.imshow(img_data)\n",
    "        plt.title(image_file)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <bound method IPythonKernel._clean_thread_parent_frames of <ipykernel.ipkernel.IPythonKernel object at 0x10565ea90>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/ipykernel/ipkernel.py\", line 770, in _clean_thread_parent_frames\n",
      "    def _clean_thread_parent_frames(\n",
      "\n",
      "KeyboardInterrupt: \n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    folder_path = california_images\n",
    "    threshold = 0.9\n",
    "    output_folder = \"../duplicates\"\n",
    "    duplicate_pairs, duplicate_image_paths, total_images = find_duplicates_sift(folder_path, threshold, output_folder)\n",
    "\n",
    "    print(f'Total images found: {total_images}')\n",
    "    print(f'Found {len(duplicate_image_paths)} individual duplicate images')\n",
    "    display_all_duplicate_images(output_folder)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
